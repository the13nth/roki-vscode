# Vector Search Implementation Guide

## Overview

The vector search implementation has been significantly improved to provide robust semantic search capabilities for analysis functionality. The system now uses real embeddings generated by Google Gemini's text-embedding-004 model and provides comprehensive context retrieval for AI-powered analysis.

## Key Improvements

### 1. **Real Embedding Generation**
- **Before**: Hash-based fallback vectors
- **After**: Google Gemini embeddings (768 → 1024 dimensions)
- **Benefits**: True semantic similarity, better search results

### 2. **Enhanced Analysis Context Retrieval**
- **New Service**: `AnalysisVectorService` for analysis-specific operations
- **Features**: 
  - Semantic search across project documents
  - Related analysis discovery
  - Context formatting for AI consumption

### 3. **Robust Error Handling**
- **Fallback Strategy**: Graceful degradation to hash-based vectors
- **Retry Logic**: Exponential backoff with circuit breaker
- **Caching**: 24-hour TTL with LRU eviction

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    Vector Search Stack                      │
├─────────────────────────────────────────────────────────────┤
│  AnalysisVectorService  │  PineconeOperationsService       │
│  - Context retrieval    │  - Core vector operations        │
│  - Related analysis     │  - Batch processing              │
│  - Formatting          │  - Error handling                 │
├─────────────────────────────────────────────────────────────┤
│  EmbeddingService       │  PineconeUtils                   │
│  - Gemini integration   │  - Retry logic                   │
│  - Caching             │  - Circuit breaker                │
│  - Fallback generation │  - Performance metrics            │
├─────────────────────────────────────────────────────────────┤
│  Pinecone Database      │  Google Gemini API               │
│  - Vector storage       │  - Embedding generation          │
│  - Namespace org        │  - text-embedding-004 model      │
└─────────────────────────────────────────────────────────────┘
```

## Usage Examples

### 1. **Basic Analysis Context Retrieval**

```typescript
import { analysisVectorService } from '@/lib/analysisVectorService';

// Get comprehensive context for technical analysis
const context = await analysisVectorService.getAnalysisContext(
  'project-123',
  'technical',
  'system architecture performance'
);

console.log(`Found ${context.relevantDocuments.length} relevant documents`);
console.log(`Found ${context.relatedAnalyses.length} related analyses`);
```

### 2. **Search for Specific Information**

```typescript
// Search for specific technical information
const results = await analysisVectorService.searchProjectInformation(
  'database optimization caching strategies',
  'project-123',
  ['requirements', 'design', 'context'],
  5
);

results.forEach(result => {
  console.log(`${result.metadata.title}: ${result.score.toFixed(3)}`);
});
```

### 3. **Get Related Analyses**

```typescript
// Find related analyses for cross-referencing
const related = await analysisVectorService.getRelatedAnalyses(
  'technical',
  'project-123',
  'current-analysis-id', // exclude current analysis
  3
);
```

### 4. **API Endpoints**

#### Get Analysis Context
```bash
GET /api/projects/{id}/analyses/context?type=technical&query=performance
```

Response:
```json
{
  "success": true,
  "context": {
    "relevantDocuments": [...],
    "relatedAnalyses": [...],
    "projectInfo": [...],
    "formattedContext": "# Project Information\n...",
    "totalDocuments": 15
  }
}
```

#### Search Analysis Information
```bash
POST /api/projects/{id}/analyses/context
{
  "query": "database optimization",
  "analysisType": "technical",
  "documentTypes": ["requirements", "design"],
  "maxResults": 10
}
```

## Configuration

### Environment Variables

```bash
# Required
GOOGLE_AI_API_KEY=your_gemini_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=your_index_name

# Optional
NEXT_PUBLIC_PINECONE_API_KEY=your_public_key
NEXT_PUBLIC_PINECONE_INDEX_NAME=your_public_index
```

### Embedding Service Configuration

```typescript
// Cache settings
CACHE_TTL = 24 * 60 * 60 * 1000; // 24 hours
MAX_CACHE_SIZE = 1000; // Maximum cached embeddings
BATCH_SIZE = 10; // Batch size for embedding generation

// Retry settings
MAX_RETRIES = 3;
TIMEOUT = 120000; // 2 minutes for embedding operations
```

## Performance Optimizations

### 1. **Embedding Caching**
- **Strategy**: LRU cache with 24-hour TTL
- **Benefit**: Avoid regenerating embeddings for identical content
- **Monitoring**: Cache hit rates and size metrics

### 2. **Batch Processing**
- **Vector Operations**: 100 vectors per batch
- **Embedding Generation**: 10 texts per batch
- **Parallel Execution**: 3-5 batches in parallel

### 3. **Circuit Breaker**
- **Threshold**: 5 consecutive failures
- **Timeout**: 60 seconds before half-open
- **Benefit**: Prevents cascade failures

## Error Handling

### 1. **Embedding Generation Failures**
```typescript
try {
  const embedding = await embeddingService.generateEmbedding(text);
} catch (error) {
  // Automatic fallback to hash-based vector
  const fallback = embeddingService.generateFallbackEmbedding(text);
}
```

### 2. **Pinecone Connection Issues**
```typescript
// Automatic retry with exponential backoff
const result = await pineconeOperationsService.query(vector, {
  maxRetries: 3,
  timeout: 45000
});
```

### 3. **Circuit Breaker Protection**
```typescript
// Operations are blocked when circuit is open
if (circuitBreaker.isOpen()) {
  throw new Error('Service temporarily unavailable');
}
```

## Monitoring and Metrics

### 1. **Performance Metrics**
```typescript
const metrics = pineconeOperationsService.getMetrics();
console.log({
  totalOperations: metrics.totalOperations,
  successRate: metrics.successRate,
  averageResponseTime: metrics.averageResponseTime
});
```

### 2. **Cache Statistics**
```typescript
const cacheStats = embeddingService.getCacheStats();
console.log({
  cacheSize: cacheStats.size,
  oldestEntry: cacheStats.oldestEntry,
  newestEntry: cacheStats.newestEntry
});
```

### 3. **Health Checks**
```typescript
const health = await pineconeOperationsService.healthCheck();
console.log({
  healthy: health.healthy,
  latency: health.latency,
  error: health.error
});
```

## Best Practices

### 1. **Query Optimization**
- Use specific, descriptive queries
- Include relevant keywords and context
- Limit result sets with appropriate `maxResults`

### 2. **Context Management**
- Combine multiple search strategies
- Use formatted context for AI consumption
- Cache frequently accessed context

### 3. **Error Handling**
- Always implement fallback strategies
- Monitor error rates and patterns
- Use circuit breakers for external services

### 4. **Performance**
- Batch operations when possible
- Use caching for repeated queries
- Monitor embedding generation costs

## Troubleshooting

### Common Issues

1. **Embedding Generation Failures**
   - Check Google AI API key and quota
   - Verify network connectivity
   - Monitor API rate limits

2. **Pinecone Connection Issues**
   - Verify API key and index name
   - Check network connectivity
   - Monitor Pinecone service status

3. **Poor Search Results**
   - Verify embedding quality
   - Check metadata structure
   - Review query specificity

### Debug Mode

```typescript
// Enable detailed logging
process.env.DEBUG_VECTOR_SEARCH = 'true';

// Check service health
const health = await pineconeOperationsService.healthCheck();
console.log('Service Health:', health);
```

## Migration Guide

### From Hash-based to Real Embeddings

1. **Update Existing Data**
   ```typescript
   // Re-embed existing documents
   const documents = await getExistingDocuments();
   for (const doc of documents) {
     const newEmbedding = await embeddingService.generateEmbedding(doc.content);
     await pineconeOperationsService.upsert([{
       id: doc.id,
       values: newEmbedding,
       metadata: doc.metadata
     }]);
   }
   ```

2. **Update Endpoints**
   - Replace hash-based vector generation
   - Use `embeddingService.generateEmbeddingWithFallback()`
   - Add proper error handling

3. **Test Search Quality**
   - Compare search results before/after
   - Verify semantic similarity improvements
   - Monitor performance metrics

## Future Enhancements

1. **Multi-modal Embeddings**
   - Support for images and documents
   - Cross-modal search capabilities

2. **Advanced Filtering**
   - Date range filters
   - User-specific filtering
   - Content type prioritization

3. **Real-time Updates**
   - Live embedding updates
   - Incremental indexing
   - Change notifications

4. **Analytics Dashboard**
   - Search analytics
   - Performance monitoring
   - Usage patterns
